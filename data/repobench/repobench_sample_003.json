{
  "repo_name": "ali-vilab/dreamtalk",
  "file_path": "core/networks/dynamic_fc_decoder.py",
  "context": [
    {
      "identifier": "_get_activation_fn",
      "path": "core/networks/transformer.py",
      "snippet": "def _get_activation_fn(activation):\r\n    \"\"\"Return an activation function given a string\"\"\"\r\n    if activation == \"relu\":\r\n        return F.relu\r\n    if activation == \"gelu\":\r\n        return F.gelu\r\n    if activation == \"glu\":\r\n        return F.glu\r\n    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\r"
    },
    {
      "identifier": "_get_clones",
      "path": "core/networks/transformer.py",
      "snippet": "def _get_clones(module, N):\r\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\r"
    },
    {
      "identifier": "DynamicLinear",
      "path": "core/networks/dynamic_linear.py",
      "snippet": "class DynamicLinear(nn.Module):\n    def __init__(self, in_planes, out_planes, cond_planes, bias=True, K=4, temperature=30, ratio=4, init_weight=True):\n        super().__init__()\n\n        self.dynamic_conv = DynamicConv(\n            in_planes,\n            out_planes,\n            cond_planes,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias,\n            K=K,\n            ratio=ratio,\n            temperature=temperature,\n            init_weight=init_weight,\n        )\n\n    def forward(self, x, cond):\n        \"\"\"\n\n        Args:\n            x (_type_): (L, B, C_in)\n            cond (_type_): (B, C_style)\n\n        Returns:\n            _type_: (L, B, C_out)\n        \"\"\"\n        x = x.permute(1, 2, 0).unsqueeze(-1)\n        out = self.dynamic_conv(x, cond)\n        # (B, C_out, L, 1)\n        out = out.squeeze().permute(2, 0, 1)\n        return out"
    }
  ],
  "import_statement": "import torch.nn as nn\nimport torch\nfrom core.networks.transformer import _get_activation_fn, _get_clones\nfrom core.networks.dynamic_linear import DynamicLinear",
  "token_num": 1476,
  "cropped_code": "\n\n\nclass DynamicFCDecoderLayer(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        d_style,\n        dynamic_K,\n        dynamic_ratio,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"relu\",\n        normalize_before=False,\n    ):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        # self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.linear1 = DynamicLinear(d_model, dim_feedforward, d_style, K=dynamic_K, ratio=dynamic_ratio)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        # self.linear2 = DynamicLinear(dim_feedforward, d_model, d_style, K=dynamic_K, ratio=dynamic_ratio)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n\n    def with_pos_embed(self, tensor, pos):\n        return tensor if pos is None else tensor + pos\n\n    def forward_post(\n        self,\n        tgt,\n        memory,\n        style,\n        tgt_mask=None,\n        memory_mask=None,\n        tgt_key_padding_mask=None,\n        memory_key_padding_mask=None,\n        pos=None,\n        query_pos=None,\n    ):\n        # q = k = self.with_pos_embed(tgt, query_pos)\n        tgt2 = self.self_attn(tgt, tgt, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(\n            query=tgt, key=memory, value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask\n        )[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        # tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt, style))), style)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt, style))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt\n\n    # def forward_pre(\n    #     self,\n    #     tgt,\n    #     memory,\n    #     tgt_mask=None,\n    #     memory_mask=None,\n    #     tgt_key_padding_mask=None,\n    #     memory_key_padding_mask=None,\n    #     pos=None,\n    #     query_pos=None,\n    # ):\n    #     tgt2 = self.norm1(tgt)\n    #     # q = k = self.with_pos_embed(tgt2, query_pos)\n    #     tgt2 = self.self_attn(tgt2, tgt2, value=tgt2, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n    #     tgt = tgt + self.dropout1(tgt2)\n    #     tgt2 = self.norm2(tgt)\n    #     tgt2 = self.multihead_attn(\n    #         query=tgt2, key=memory, value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask\n    #     )[0]\n    #     tgt = tgt + self.dropout2(tgt2)\n    #     tgt2 = self.norm3(tgt)\n    #     tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n    #     tgt = tgt + self.dropout3(tgt2)\n    #     return tgt\n\n    def forward(\n        self,\n        tgt,\n        memory,\n        style,\n        tgt_mask=None,\n        memory_mask=None,\n        tgt_key_padding_mask=None,\n        memory_key_padding_mask=None,\n        pos=None,\n        query_pos=None,\n    ):\n        if self.normalize_before:\n            raise NotImplementedError\n            # return self.forward_pre(\n            #     tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos\n            # )\n        return self.forward_post(\n            tgt, memory, style, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos\n        )\n\n\nclass DynamicFCDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n        super().__init__()\n",
  "all_code": "\n\n\nclass DynamicFCDecoderLayer(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        d_style,\n        dynamic_K,\n        dynamic_ratio,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"relu\",\n        normalize_before=False,\n    ):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        # self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.linear1 = DynamicLinear(d_model, dim_feedforward, d_style, K=dynamic_K, ratio=dynamic_ratio)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        # self.linear2 = DynamicLinear(dim_feedforward, d_model, d_style, K=dynamic_K, ratio=dynamic_ratio)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n        self.normalize_before = normalize_before\n\n    def with_pos_embed(self, tensor, pos):\n        return tensor if pos is None else tensor + pos\n\n    def forward_post(\n        self,\n        tgt,\n        memory,\n        style,\n        tgt_mask=None,\n        memory_mask=None,\n        tgt_key_padding_mask=None,\n        memory_key_padding_mask=None,\n        pos=None,\n        query_pos=None,\n    ):\n        # q = k = self.with_pos_embed(tgt, query_pos)\n        tgt2 = self.self_attn(tgt, tgt, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(\n            query=tgt, key=memory, value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask\n        )[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        # tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt, style))), style)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt, style))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt\n\n    # def forward_pre(\n    #     self,\n    #     tgt,\n    #     memory,\n    #     tgt_mask=None,\n    #     memory_mask=None,\n    #     tgt_key_padding_mask=None,\n    #     memory_key_padding_mask=None,\n    #     pos=None,\n    #     query_pos=None,\n    # ):\n    #     tgt2 = self.norm1(tgt)\n    #     # q = k = self.with_pos_embed(tgt2, query_pos)\n    #     tgt2 = self.self_attn(tgt2, tgt2, value=tgt2, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n    #     tgt = tgt + self.dropout1(tgt2)\n    #     tgt2 = self.norm2(tgt)\n    #     tgt2 = self.multihead_attn(\n    #         query=tgt2, key=memory, value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask\n    #     )[0]\n    #     tgt = tgt + self.dropout2(tgt2)\n    #     tgt2 = self.norm3(tgt)\n    #     tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n    #     tgt = tgt + self.dropout3(tgt2)\n    #     return tgt\n\n    def forward(\n        self,\n        tgt,\n        memory,\n        style,\n        tgt_mask=None,\n        memory_mask=None,\n        tgt_key_padding_mask=None,\n        memory_key_padding_mask=None,\n        pos=None,\n        query_pos=None,\n    ):\n        if self.normalize_before:\n            raise NotImplementedError\n            # return self.forward_pre(\n            #     tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos\n            # )\n        return self.forward_post(\n            tgt, memory, style, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos\n        )\n\n\nclass DynamicFCDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n        super().__init__()",
  "next_line": "        self.layers = _get_clones(decoder_layer, num_layers)",
  "gold_snippet_index": 1,
  "created_at": "2023-12-28 05:39:31+00:00",
  "level": "2k"
}