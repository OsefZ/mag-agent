{
  "repo_name": "jiawei-ren/dreamgaussian4d",
  "file_path": "diffusers/src/diffusers/models/activations.py",
  "context": [
    {
      "identifier": "USE_PEFT_BACKEND",
      "path": "diffusers/src/diffusers/utils/constants.py",
      "snippet": "USE_PEFT_BACKEND = _required_peft_version and _required_transformers_version"
    },
    {
      "identifier": "LoRACompatibleLinear",
      "path": "diffusers/src/diffusers/models/lora.py",
      "snippet": "class LoRACompatibleLinear(nn.Linear):\n    \"\"\"\n    A Linear layer that can be used with LoRA.\n    \"\"\"\n\n    def __init__(self, *args, lora_layer: Optional[LoRALinearLayer] = None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.lora_layer = lora_layer\n\n    def set_lora_layer(self, lora_layer: Optional[LoRALinearLayer]):\n        self.lora_layer = lora_layer\n\n    def _fuse_lora(self, lora_scale: float = 1.0, safe_fusing: bool = False):\n        if self.lora_layer is None:\n            return\n\n        dtype, device = self.weight.data.dtype, self.weight.data.device\n\n        w_orig = self.weight.data.float()\n        w_up = self.lora_layer.up.weight.data.float()\n        w_down = self.lora_layer.down.weight.data.float()\n\n        if self.lora_layer.network_alpha is not None:\n            w_up = w_up * self.lora_layer.network_alpha / self.lora_layer.rank\n\n        fused_weight = w_orig + (lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0])\n\n        if safe_fusing and torch.isnan(fused_weight).any().item():\n            raise ValueError(\n                \"This LoRA weight seems to be broken. \"\n                f\"Encountered NaN values when trying to fuse LoRA weights for {self}.\"\n                \"LoRA weights will not be fused.\"\n            )\n\n        self.weight.data = fused_weight.to(device=device, dtype=dtype)\n\n        # we can drop the lora layer now\n        self.lora_layer = None\n\n        # offload the up and down matrices to CPU to not blow the memory\n        self.w_up = w_up.cpu()\n        self.w_down = w_down.cpu()\n        self._lora_scale = lora_scale\n\n    def _unfuse_lora(self):\n        if not (getattr(self, \"w_up\", None) is not None and getattr(self, \"w_down\", None) is not None):\n            return\n\n        fused_weight = self.weight.data\n        dtype, device = fused_weight.dtype, fused_weight.device\n\n        w_up = self.w_up.to(device=device).float()\n        w_down = self.w_down.to(device).float()\n\n        unfused_weight = fused_weight.float() - (self._lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0])\n        self.weight.data = unfused_weight.to(device=device, dtype=dtype)\n\n        self.w_up = None\n        self.w_down = None\n\n    def forward(self, hidden_states: torch.Tensor, scale: float = 1.0) -> torch.Tensor:\n        if self.lora_layer is None:\n            out = super().forward(hidden_states)\n            return out\n        else:\n            out = super().forward(hidden_states) + (scale * self.lora_layer(hidden_states))\n            return out"
    }
  ],
  "import_statement": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom ..utils import USE_PEFT_BACKEND\nfrom .lora import LoRACompatibleLinear",
  "token_num": 1423,
  "cropped_code": "# coding=utf-8\n# Copyright 2023 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\n\nACTIVATION_FUNCTIONS = {\n    \"swish\": nn.SiLU(),\n    \"silu\": nn.SiLU(),\n    \"mish\": nn.Mish(),\n    \"gelu\": nn.GELU(),\n    \"relu\": nn.ReLU(),\n}\n\n\ndef get_activation(act_fn: str) -> nn.Module:\n    \"\"\"Helper function to get activation function from string.\n\n    Args:\n        act_fn (str): Name of activation function.\n\n    Returns:\n        nn.Module: Activation function.\n    \"\"\"\n\n    act_fn = act_fn.lower()\n    if act_fn in ACTIVATION_FUNCTIONS:\n        return ACTIVATION_FUNCTIONS[act_fn]\n    else:\n        raise ValueError(f\"Unsupported activation function: {act_fn}\")\n\n\nclass GELU(nn.Module):\n    r\"\"\"\n    GELU activation function with tanh approximation support with `approximate=\"tanh\"`.\n\n    Parameters:\n        dim_in (`int`): The number of channels in the input.\n        dim_out (`int`): The number of channels in the output.\n        approximate (`str`, *optional*, defaults to `\"none\"`): If `\"tanh\"`, use tanh approximation.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int, approximate: str = \"none\"):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out)\n        self.approximate = approximate\n\n    def gelu(self, gate: torch.Tensor) -> torch.Tensor:\n        if gate.device.type != \"mps\":\n            return F.gelu(gate, approximate=self.approximate)\n        # mps: gelu is not implemented for float16\n        return F.gelu(gate.to(dtype=torch.float32), approximate=self.approximate).to(dtype=gate.dtype)\n\n    def forward(self, hidden_states):\n        hidden_states = self.proj(hidden_states)\n        hidden_states = self.gelu(hidden_states)\n        return hidden_states\n\n\nclass GEGLU(nn.Module):\n    r\"\"\"\n    A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation function.\n\n    Parameters:\n        dim_in (`int`): The number of channels in the input.\n        dim_out (`int`): The number of channels in the output.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int):\n        super().__init__()\n",
  "all_code": "# coding=utf-8\n# Copyright 2023 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\n\nACTIVATION_FUNCTIONS = {\n    \"swish\": nn.SiLU(),\n    \"silu\": nn.SiLU(),\n    \"mish\": nn.Mish(),\n    \"gelu\": nn.GELU(),\n    \"relu\": nn.ReLU(),\n}\n\n\ndef get_activation(act_fn: str) -> nn.Module:\n    \"\"\"Helper function to get activation function from string.\n\n    Args:\n        act_fn (str): Name of activation function.\n\n    Returns:\n        nn.Module: Activation function.\n    \"\"\"\n\n    act_fn = act_fn.lower()\n    if act_fn in ACTIVATION_FUNCTIONS:\n        return ACTIVATION_FUNCTIONS[act_fn]\n    else:\n        raise ValueError(f\"Unsupported activation function: {act_fn}\")\n\n\nclass GELU(nn.Module):\n    r\"\"\"\n    GELU activation function with tanh approximation support with `approximate=\"tanh\"`.\n\n    Parameters:\n        dim_in (`int`): The number of channels in the input.\n        dim_out (`int`): The number of channels in the output.\n        approximate (`str`, *optional*, defaults to `\"none\"`): If `\"tanh\"`, use tanh approximation.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int, approximate: str = \"none\"):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out)\n        self.approximate = approximate\n\n    def gelu(self, gate: torch.Tensor) -> torch.Tensor:\n        if gate.device.type != \"mps\":\n            return F.gelu(gate, approximate=self.approximate)\n        # mps: gelu is not implemented for float16\n        return F.gelu(gate.to(dtype=torch.float32), approximate=self.approximate).to(dtype=gate.dtype)\n\n    def forward(self, hidden_states):\n        hidden_states = self.proj(hidden_states)\n        hidden_states = self.gelu(hidden_states)\n        return hidden_states\n\n\nclass GEGLU(nn.Module):\n    r\"\"\"\n    A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation function.\n\n    Parameters:\n        dim_in (`int`): The number of channels in the input.\n        dim_out (`int`): The number of channels in the output.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int):\n        super().__init__()",
  "next_line": "        linear_cls = LoRACompatibleLinear if not USE_PEFT_BACKEND else nn.Linear",
  "gold_snippet_index": 1,
  "created_at": "2023-12-28 08:17:40+00:00",
  "level": "2k"
}