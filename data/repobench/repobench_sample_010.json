{
  "repo_name": "jesenzhang/ComfyUI_StreamDiffusion",
  "file_path": "streamdiffusion/pipeline.py",
  "context": [
    {
      "identifier": "SimilarImageFilter",
      "path": "streamdiffusion/image_filter.py",
      "snippet": "class SimilarImageFilter:\n    def __init__(self, threshold: float = 0.98, max_skip_frame: float = 10) -> None:\n        self.threshold = threshold\n        self.prev_tensor = None\n        self.cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n        self.max_skip_frame = max_skip_frame\n        self.skip_count = 0\n\n    def __call__(self, x: torch.Tensor) -> Optional[torch.Tensor]:\n        if self.prev_tensor is None:\n            self.prev_tensor = x.detach().clone()\n            return x\n        else:\n            cos_sim = self.cos(self.prev_tensor.reshape(-1), x.reshape(-1)).item()\n            sample = random.uniform(0, 1)\n            if self.threshold >= 1:\n                skip_prob = 0\n            else:\n                skip_prob = max(0, 1 - (1 - cos_sim) / (1 - self.threshold))\n\n            # not skip frame\n            if skip_prob < sample:\n                self.prev_tensor = x.detach().clone()\n                return x\n            # skip frame\n            else:\n                if self.skip_count > self.max_skip_frame:\n                    self.skip_count = 0\n                    self.prev_tensor = x.detach().clone()\n                    return x\n                else:\n                    self.skip_count += 1\n                    return None\n\n    def set_threshold(self, threshold: float) -> None:\n        self.threshold = threshold\n    \n    def set_max_skip_frame(self, max_skip_frame: float) -> None:\n        self.max_skip_frame = max_skip_frame"
    },
    {
      "identifier": "postprocess_image",
      "path": "streamdiffusion/image_utils.py",
      "snippet": "def postprocess_image(\n    image: torch.Tensor,\n    output_type: str = \"pil\",\n    do_denormalize: Optional[List[bool]] = None,\n) -> Union[torch.Tensor, np.ndarray, PIL.Image.Image]:\n    if not isinstance(image, torch.Tensor):\n        raise ValueError(\n            f\"Input for postprocessing is in incorrect format: {type(image)}. We only support pytorch tensor\"\n        )\n\n    if output_type == \"latent\":\n        return image\n\n    do_normalize_flg = True\n    if do_denormalize is None:\n        do_denormalize = [do_normalize_flg] * image.shape[0]\n\n    image = torch.stack(\n        [\n            denormalize(image[i]) if do_denormalize[i] else image[i]\n            for i in range(image.shape[0])\n        ]\n    )\n\n    if output_type == \"pt\":\n        return image\n\n    image = pt_to_numpy(image)\n\n    if output_type == \"np\":\n        return image\n\n    if output_type == \"pil\":\n        return numpy_to_pil(image)"
    }
  ],
  "import_statement": "import time\nimport numpy as np\nimport PIL.Image\nimport torch\nfrom typing import List, Optional, Union, Any, Dict, Tuple, Literal\nfrom diffusers import LCMScheduler, StableDiffusionPipeline\nfrom diffusers.image_processor import VaeImageProcessor\nfrom diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img import (\n    retrieve_latents,\n)\nfrom .image_filter import SimilarImageFilter\nfrom .image_utils import postprocess_image",
  "token_num": 1162,
  "cropped_code": "\n\n\n\nclass StreamDiffusion:\n    def __init__(\n        self,\n        pipe: StableDiffusionPipeline,\n        t_index_list: List[int],\n        torch_dtype: torch.dtype = torch.float16,\n        width: int = 512,\n        height: int = 512,\n        do_add_noise: bool = True,\n        use_denoising_batch: bool = True,\n        frame_buffer_size: int = 1,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    ) -> None:\n        self.device = pipe.device\n        self.dtype = torch_dtype\n        self.generator = None\n\n        self.height = height\n        self.width = width\n\n        self.latent_height = int(height // pipe.vae_scale_factor)\n        self.latent_width = int(width // pipe.vae_scale_factor)\n\n        self.frame_bff_size = frame_buffer_size\n        self.denoising_steps_num = len(t_index_list)\n\n        self.cfg_type = cfg_type\n\n        if use_denoising_batch:\n            self.batch_size = self.denoising_steps_num * frame_buffer_size\n            if self.cfg_type == \"initialize\":\n                self.trt_unet_batch_size = (\n                    self.denoising_steps_num + 1\n                ) * self.frame_bff_size\n            elif self.cfg_type == \"full\":\n                self.trt_unet_batch_size = (\n                    2 * self.denoising_steps_num * self.frame_bff_size\n                )\n            else:\n                self.trt_unet_batch_size = self.denoising_steps_num * frame_buffer_size\n        else:\n            self.trt_unet_batch_size = self.frame_bff_size\n            self.batch_size = frame_buffer_size\n\n        self.t_list = t_index_list\n\n        self.do_add_noise = do_add_noise\n        self.use_denoising_batch = use_denoising_batch\n\n        self.similar_image_filter = False\n",
  "all_code": "\n\n\n\nclass StreamDiffusion:\n    def __init__(\n        self,\n        pipe: StableDiffusionPipeline,\n        t_index_list: List[int],\n        torch_dtype: torch.dtype = torch.float16,\n        width: int = 512,\n        height: int = 512,\n        do_add_noise: bool = True,\n        use_denoising_batch: bool = True,\n        frame_buffer_size: int = 1,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    ) -> None:\n        self.device = pipe.device\n        self.dtype = torch_dtype\n        self.generator = None\n\n        self.height = height\n        self.width = width\n\n        self.latent_height = int(height // pipe.vae_scale_factor)\n        self.latent_width = int(width // pipe.vae_scale_factor)\n\n        self.frame_bff_size = frame_buffer_size\n        self.denoising_steps_num = len(t_index_list)\n\n        self.cfg_type = cfg_type\n\n        if use_denoising_batch:\n            self.batch_size = self.denoising_steps_num * frame_buffer_size\n            if self.cfg_type == \"initialize\":\n                self.trt_unet_batch_size = (\n                    self.denoising_steps_num + 1\n                ) * self.frame_bff_size\n            elif self.cfg_type == \"full\":\n                self.trt_unet_batch_size = (\n                    2 * self.denoising_steps_num * self.frame_bff_size\n                )\n            else:\n                self.trt_unet_batch_size = self.denoising_steps_num * frame_buffer_size\n        else:\n            self.trt_unet_batch_size = self.frame_bff_size\n            self.batch_size = frame_buffer_size\n\n        self.t_list = t_index_list\n\n        self.do_add_noise = do_add_noise\n        self.use_denoising_batch = use_denoising_batch\n\n        self.similar_image_filter = False",
  "next_line": "        self.similar_filter = SimilarImageFilter()",
  "gold_snippet_index": 0,
  "created_at": "2023-12-29 09:00:03+00:00",
  "level": "2k"
}