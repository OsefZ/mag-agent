{
  "repo_name": "Meituan-AutoML/MobileVLM",
  "file_path": "mobilevlm/model/mobilevlm.py",
  "context": [
    {
      "identifier": "build_vision_tower",
      "path": "mobilevlm/model/vision_encoder.py",
      "snippet": "def build_vision_tower(model_cfg, **kwargs):\n    vision_tower = getattr(model_cfg, 'mm_vision_tower', getattr(model_cfg, 'vision_tower', None))\n    is_absolute_path_exists = os.path.exists(vision_tower)\n    if is_absolute_path_exists or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\"):\n        vision_tower_type = getattr(model_cfg, 'vision_tower_type', None)\n        if vision_tower_type == \"clip\":\n            return CLIPVisionTower(vision_tower, args=model_cfg, **kwargs)\n    raise ValueError(f'Unknown vision tower: {vision_tower}')"
    },
    {
      "identifier": "build_vision_projector",
      "path": "mobilevlm/model/vision_projector.py",
      "snippet": "def build_vision_projector(config, delay_load=False, **kwargs):\n    projector_type = getattr(config, 'mm_projector_type', 'linear')\n\n    if projector_type == 'linear':\n        return nn.Linear(config.mm_hidden_size, config.hidden_size)\n    elif projector_type.startswith('mlp'):\n        mlp_gelu_match = re.match(r'^mlp(\\d+)x_gelu$', projector_type)\n        if mlp_gelu_match:\n            mlp_depth = int(mlp_gelu_match.group(1))\n            modules = [nn.Linear(config.mm_hidden_size, config.hidden_size)]\n            for _ in range(1, mlp_depth):\n                modules.append(nn.GELU())\n                modules.append(nn.Linear(config.hidden_size, config.hidden_size))\n            return nn.Sequential(*modules)\n    elif projector_type.startswith('ldpnet'):\n        return LDPNetProjector(config)\n    raise ValueError(f'Unknown projector type: {projector_type}')"
    },
    {
      "identifier": "IGNORE_INDEX",
      "path": "mobilevlm/constants.py",
      "snippet": "IGNORE_INDEX = -100"
    },
    {
      "identifier": "IMAGE_TOKEN_INDEX",
      "path": "mobilevlm/constants.py",
      "snippet": "IMAGE_TOKEN_INDEX = -200"
    },
    {
      "identifier": "DEFAULT_IMAGE_PATCH_TOKEN",
      "path": "mobilevlm/constants.py",
      "snippet": "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\""
    },
    {
      "identifier": "DEFAULT_IM_START_TOKEN",
      "path": "mobilevlm/constants.py",
      "snippet": "DEFAULT_IM_START_TOKEN = \"<im_start>\""
    },
    {
      "identifier": "DEFAULT_IM_END_TOKEN",
      "path": "mobilevlm/constants.py",
      "snippet": "DEFAULT_IM_END_TOKEN = \"<im_end>\""
    }
  ],
  "import_statement": "import torch\nimport torch.nn as nn\nfrom abc import ABC, abstractmethod\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom mobilevlm.model.vision_encoder import build_vision_tower\nfrom mobilevlm.model.vision_projector import build_vision_projector\nfrom mobilevlm.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, \\\n    DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n    from mobilevlm.model.mobilellama import MobileLlamaForCausalLM",
  "token_num": 1423,
  "cropped_code": "\n\nclass MobileVLMMetaModel:\n\n    def __init__(self, config):\n        super(MobileVLMMetaModel, self).__init__(config)\n        if hasattr(config, \"mm_vision_tower\"):  \n            self.vision_tower = build_vision_tower(config, delay_load=False)\n            self.mm_projector = build_vision_projector(config)\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        self.config.mm_vision_tower = model_args.vision_tower\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        # Build VisionTower\n        vision_tower = build_vision_tower(model_args)\n        if fsdp is not None and len(fsdp) > 0:\n            self.vision_tower = [vision_tower]\n        else:\n            self.vision_tower = vision_tower\n        self.config.mm_hidden_size = vision_tower.hidden_size\n        # Build Vision-Projector\n        self.mm_projector = build_vision_projector(self.config)\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n            def get_w(weights, keyword):\n                return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n            self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\n\n\nclass MobileVLMMetaForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n\n    def prepare_inputs_labels_for_multimodal(\n        self, input_ids, attention_mask, past_key_values, labels, images\n    ):\n        vision_tower = self.get_vision_tower()\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            if past_key_values is not None and vision_tower is not None and images is not None and input_ids.shape[1] == 1:\n                attention_mask = torch.ones((attention_mask.shape[0], past_key_values[-1][-1].shape[-2] + 1), dtype=attention_mask.dtype, device=attention_mask.device)\n            return input_ids, attention_mask, past_key_values, None, labels\n\n        if type(images) is list or images.ndim == 5:\n            concat_images = torch.cat([image for image in images], dim=0)\n            image_features = self.encode_images(concat_images)\n            split_sizes = [image.shape[0] for image in images]\n            image_features = torch.split(image_features, split_sizes, dim=0)\n            image_features = [x.flatten(0, 1) for x in image_features]\n        else:\n            image_features = self.encode_images(images)\n\n        new_input_embeds = []\n        new_labels = [] if labels is not None else None\n        cur_image_idx = 0\n        for batch_idx, cur_input_ids in enumerate(input_ids):\n",
  "all_code": "\n\nclass MobileVLMMetaModel:\n\n    def __init__(self, config):\n        super(MobileVLMMetaModel, self).__init__(config)\n        if hasattr(config, \"mm_vision_tower\"):  \n            self.vision_tower = build_vision_tower(config, delay_load=False)\n            self.mm_projector = build_vision_projector(config)\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        self.config.mm_vision_tower = model_args.vision_tower\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        # Build VisionTower\n        vision_tower = build_vision_tower(model_args)\n        if fsdp is not None and len(fsdp) > 0:\n            self.vision_tower = [vision_tower]\n        else:\n            self.vision_tower = vision_tower\n        self.config.mm_hidden_size = vision_tower.hidden_size\n        # Build Vision-Projector\n        self.mm_projector = build_vision_projector(self.config)\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n            def get_w(weights, keyword):\n                return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n            self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\n\n\nclass MobileVLMMetaForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n\n    def prepare_inputs_labels_for_multimodal(\n        self, input_ids, attention_mask, past_key_values, labels, images\n    ):\n        vision_tower = self.get_vision_tower()\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            if past_key_values is not None and vision_tower is not None and images is not None and input_ids.shape[1] == 1:\n                attention_mask = torch.ones((attention_mask.shape[0], past_key_values[-1][-1].shape[-2] + 1), dtype=attention_mask.dtype, device=attention_mask.device)\n            return input_ids, attention_mask, past_key_values, None, labels\n\n        if type(images) is list or images.ndim == 5:\n            concat_images = torch.cat([image for image in images], dim=0)\n            image_features = self.encode_images(concat_images)\n            split_sizes = [image.shape[0] for image in images]\n            image_features = torch.split(image_features, split_sizes, dim=0)\n            image_features = [x.flatten(0, 1) for x in image_features]\n        else:\n            image_features = self.encode_images(images)\n\n        new_input_embeds = []\n        new_labels = [] if labels is not None else None\n        cur_image_idx = 0\n        for batch_idx, cur_input_ids in enumerate(input_ids):",
  "next_line": "            if (cur_input_ids == IMAGE_TOKEN_INDEX).sum() == 0:",
  "gold_snippet_index": 3,
  "created_at": "2023-12-29 03:35:49+00:00",
  "level": "2k"
}